{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Lab - NERC Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the deliverables for the AHLT Lab NERC Baseline assignment, corresponding to Goals 1 and 2.\n",
    "The notebook contains the following sections:\n",
    "\n",
    "- [Entity extractor function *extract entities*](#function), with subset of features function to achieve Goals 1 and 2.\n",
    "    \n",
    "- [Evaluator output for Devel/Test sets for Goal 1.](#output_1)\n",
    "- [Evaluator output for Devel/Test sets for Goal 2.](#output_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='function'></a>\n",
    "## Entity extractor function *extract_entities*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better detect the different kinds of entities, by devised rules that characterise each type's tokens, on average, from the others. For instance, upper cased tokens are more probable to be brands.\n",
    "\n",
    "This way of creating rules allows for fast improvement of the F1 score up to a certain point, given the rules are not absolute and each one may introduce incorrect detections.\n",
    "\n",
    "Our most successful rules were the ones with regards to the suffixes/prefixes, where we took the most common 5-character suffixes/prefixes for each type and match if the token ends/begins with those strings. As the frequencies of each suffix/prefix were taken from the *Train* data-set entities, we expect them to generalise worse than other rules for the *Test* data-set, given the samples in Train data-set do not have the same distribution of suffixes/prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(token_list):\n",
    "    \"\"\"\n",
    "    Extract entitites.\n",
    "    Fuction to extract and tag the entites of the give token lists, taggin each\n",
    "    foun entity with a type given a set of rules.\n",
    "\n",
    "    Args:\n",
    "        - token_list: list of token strings with token words\n",
    "    Returns:\n",
    "        - ents: list of dictionaries with entities' name, type and offset.\n",
    "    \"\"\"\n",
    "    # For use in entity recognition rules\n",
    "    # Common drug suffixes\n",
    "    with open(\"drug_suffixes.txt\", \"r\") as fp:\n",
    "        drug_suffixes = [s.replace(\"\\n\", \"\") for s in fp.readlines()]\n",
    "    with open(\"group_suffixes.txt\", \"r\") as fp:\n",
    "        group_suffixes = [s.replace(\"\\n\", \"\") for s in fp.readlines()]\n",
    "    with open(\"brand_suffixes.txt\", \"r\") as fp:\n",
    "        brand_suffixes = [s.replace(\"\\n\", \"\") for s in fp.readlines()]\n",
    "    with open(\"drug_n_suffixes.txt\", \"r\") as fp:\n",
    "        drug_n_suffixes = [s.replace(\"\\n\", \"\") for s in fp.readlines()]\n",
    "    with open(\"data/Rules/drug_prefixes.txt\", \"r\") as fp:\n",
    "        drug_prefixes = [s.replace(\"\\n\", \"\") for s in fp.readlines()]\n",
    "    with open(\"data/Rules/group_prefixes.txt\", \"r\") as fp:\n",
    "        group_prefixes = [s.replace(\"\\n\", \"\") for s in fp.readlines()]\n",
    "    with open(\"data/Rules/brand_prefixes.txt\", \"r\") as fp:\n",
    "        brand_prefixes = [s.replace(\"\\n\", \"\") for s in fp.readlines()]\n",
    "    with open(\"data/Rules/drug_n_prefixes.txt\", \"r\") as fp:\n",
    "        drug_n_prefixes = [s.replace(\"\\n\", \"\") for s in fp.readlines()]\n",
    "    # Init output list\n",
    "    ents = []\n",
    "    # Iterate over index of token_list and detect entity type with rules\n",
    "    # if no entity type detected, token is discarded and next token evaluated.\n",
    "    i = 0\n",
    "    i_max = len(token_list)\n",
    "    while i < i_max:\n",
    "        # Take token info\n",
    "        token, start, end = token_list[i]\n",
    "        # We take next token and previous token (if any) to evaluate\n",
    "        # composite entity names.\n",
    "        nxt_token, nxt_stat, nxt_end = token_list[i+1] if i < (i_max-1) else \\\n",
    "            (\"EOS\", inf, inf)\n",
    "        prv_token, prv_stat, prv_end = token_list[i-1] if i > 0 else \\\n",
    "            (\"BOS\", inf, inf)\n",
    "        type = None\n",
    "        # Rules to detect if token is entity and which type\n",
    "        # Detect \"XX acid\" drugs\n",
    "        if nxt_token == \"acid\":\n",
    "            type = \"drug\"\n",
    "            token = f\"{token} {nxt_token}\"\n",
    "            end = nxt_end\n",
    "            i += 1\n",
    "        # Detect \"XX agents\", \"XX drugs\" and \"XX drug\" groups\n",
    "        elif ((nxt_token == \"agents\") or (nxt_token == \"drugs\")):\n",
    "            type = \"group\"\n",
    "            token = f\"{token} {nxt_token}\"\n",
    "            end = nxt_end\n",
    "            i += 1\n",
    "        # Detect drug_n with usual suffixes\n",
    "        elif (\n",
    "              token.endswith(tuple(drug_n_suffixes)) or\n",
    "              token.startswith(tuple(drug_n_prefixes))\n",
    "              # Features for Goal 2\n",
    "              # Tokens of type drug_n have on average more number of\n",
    "              # non-alphanumeric symbols.\n",
    "              or sum(w in [\"+\", \"-\", \"(\", \")\"] for w in token)\n",
    "              ):\n",
    "            type = \"drug_n\"\n",
    "        # Detect common brand suffixes\n",
    "        elif (\n",
    "            token.endswith(tuple(brand_suffixes)) or\n",
    "            token.startswith(tuple(group_prefixes))\n",
    "            # Features for Goal 2\n",
    "            # Uppercase brand names\n",
    "            # Avoid numerals and acronyms by limiting length\n",
    "            or token.isupper() and len(token) > 4\n",
    "            ):\n",
    "            type = \"brand\"\n",
    "        # Detect common group suffixes\n",
    "        elif (\n",
    "            token.endswith(tuple(group_suffixes)) or\n",
    "            token.startswith(tuple(group_prefixes))\n",
    "            # Features for Goal 2\n",
    "            # Detect plural acronyms i.e. ADs\n",
    "            or match(r\"[A-Z]+s$\", token)\n",
    "            ):\n",
    "            type = \"group\"\n",
    "        # Detect common drug suffixes\n",
    "        elif (\n",
    "            token.endswith(tuple(drug_suffixes)) or \n",
    "            token.startswith(tuple(drug_prefixes))\n",
    "            ):\n",
    "            type = \"drug\"\n",
    "        # If type was set, then it's an entity\n",
    "        if type is not None:\n",
    "            ent = {\"name\": token, \"offset\": f\"{start}-{end}\", \"type\": type}\n",
    "            ents.append(ent)\n",
    "        # Pass to next token\n",
    "        i += 1\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_1'></a>\n",
    "## Evaluator output for Devel/Test sets for Goal 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Devel data-set with features indicated to obtain Goal 1\n",
    "\n",
    "With the subset of features indicated in the previous section as minimal features, we obtain a F1 average score of 0.5 with the Devel data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE \n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "787|349|0|635|396|1771|0.51|0.44|0.48\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "934|202|0|635|396|1771|0.61|0.53|0.57\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "934|0|202|635|396|1771|0.61|0.58|0.6\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "913|223|0|635|396|1771|0.6|0.52|0.55\n",
    "\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "494|17|0|534|48|1045|0.88|0.47|0.62\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "112|3|0|65|38|180|0.73|0.62|0.67\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "164|106|0|184|98|454|0.45|0.36|0.4\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "17|0|0|75|7|92|0.71|0.18|0.29\n",
    "\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.69|0.41|0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see **drug** and **brand** have F1 scores over **0.6**, meaning our rules capture well this types of entities, while for **group** and **drug_n** we have much lower scores of **0.4** and **0.39**. This was to be expected since these last types have more common multiple token entities that are not well detected through our rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Test data-set with features indicated to obtain Goal 1\n",
    "\n",
    "We know apply these minimal features to the Test data-set to see how well they generalise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE \n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "252|102|0|332|234|686|0.43|0.37|0.4\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "292|62|0|332|234|686|0.5|0.43|0.46\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "292|0|62|332|234|686|0.5|0.47|0.48\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "293|61|0|332|234|686|0.5|0.43|0.46\n",
    "\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "164|18|0|169|29|351|0.78|0.47|0.58\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "32|0|0|27|7|59|0.82|0.54|0.65\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "56|16|0|83|55|155|0.44|0.36|0.4\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "0|7|0|114|1|121|0|0|0\n",
    "\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.51|0.34|0.41\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, when applying the extracted rules for recognizing and classifying our entities to the Test data-set, we realize the metrics go down below the intended threshold. This is due to the fact that our rules overfit the data in our development data-set but have a big generalization error, and thus do not apply well in the general case.\n",
    "\n",
    "In particular, we see the greatest deviation with the validation metrics in the F1 score for **drug_n**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_2'></a>\n",
    "## Evaluator output for Devel/Test sets for Goal 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Devel data-set with features indicated to obtain Goal 2\n",
    "\n",
    "In this case we add the extra features to achieve the maximum F1 score on the Devel data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE RUN=1\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|797|441|0|533|664|1771|0.42|0.45|0.43|\n",
    "\n",
    "Exact matching\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|930|308|0|533|664|1771|0.49|0.53|0.51|\n",
    "\n",
    "Partial matching\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|930|0|308|533|664|1771|0.49|0.61|0.54|\n",
    "\n",
    "type matching\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|907|331|0|533|664|1771|0.48|0.51|0.49|\n",
    "\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|485|10|0|550|42|1045|0.9|0.46|0.61|\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|111|0|0|69|26|180|0.81|0.62|0.7|\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|147|91|0|216|59|454|0.49|0.32|0.39|\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|54|9|0|29|70|92|0.41|0.59|0.48|\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "|P|R|F1|\n",
    "|--|--|--|\n",
    "|0.65|0.5|0.55|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous section, we know have a significantly better F1 for the previous lower types **group** and **drug_n**, with a higher score for **brand** too, while not changing the F1 score for **drug**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Test data-set with features indicated to obtain Goal 2\n",
    "\n",
    "Like before, we see how the extra features generalise with the Test data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE RUN=2\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "255|146|0|285|401|686|0.32|0.37|0.34\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "294|107|0|285|401|686|0.37|0.43|0.4\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "294|0|107|285|401|686|0.37|0.51|0.43\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "311|90|0|285|401|686|0.39|0.45|0.42\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "164|7|0|180|28|351|0.82|0.47|0.6\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "32|0|0|27|2|59|0.94|0.54|0.69\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "49|15|0|91|30|155|0.52|0.32|0.39\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "10|35|0|76|73|121|0.08|0.08|0.08\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.59|0.35|0.44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we obtain a significantly bettwe score, we still have a high generalization error. Again, being the main discrepancy the F1 score for **drug_n** type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
