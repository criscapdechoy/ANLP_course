{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Lab - NERC ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the deliverables for the AHLT Lab NERC Machine Learning assignment, corresponding to Goals 3 and 4.\n",
    "The notebook contains the following sections:\n",
    "\n",
    "- [Feature extractor function *extract features*](#features), with subset of features function to achieve Goals 3 and 4.\n",
    "- [Classifier function *classifier*](#classifier)\n",
    "- [Output generator function *output entities*](#output)\n",
    "- [Evaluator output for Devel/Test sets for Goal 3.](#output_1)\n",
    "- [Evaluator output for Devel/Test sets for Goal 4.](#output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features'></a>\n",
    "## Feature extractor function *extract_features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve upon the baseline entity classification throughy ruled-based entity recognition, we devise a set of features with which we train three different classifiers to detect our entities.\n",
    "\n",
    "The threee classifiers we used are:\n",
    "\n",
    "- **Conditonal Random Fields classifier** (**CRF**), through its implementation *pycrfsuite* Python package.\n",
    "- **Maximum Entropy classifier** (**MaxEnt**), through its implementation as a command line executable, details in [here](http://users.umiacs.umd.edu/~hal/megam/version0_3/)\n",
    "- **Random Forest classifier** (**RandomForest**), through its implementation in *Sklearn* Python package.\n",
    "\n",
    "For each classifier, we devise a set features that characterise the different tokens to classify them into drug entity types and using the B-I-O rules. That is we have 9 classes to classify tokens into: eight **B**(egining) or **I**(nternal) tags for each type (i.e. B-drug, I-group, etc); and a **O** tag for non-entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(token_list):\n",
    "    \"\"\"\n",
    "    Extract Features\n",
    "    Fuction to extract features from each token of the given token list.\n",
    "    Args:\n",
    "        - token_list: list of token strings with token words\n",
    "    Returns:\n",
    "        - features: list of list of features for each token of the given list.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for i, token_t in enumerate(token_list):\n",
    "        token, start, end = token_t\n",
    "        # Token form\n",
    "        form = f\"form={token}\"\n",
    "        # Suffix's 4 last letters\n",
    "        suf4 = token[-4:]\n",
    "        suf4 = f\"suf4={suf4}\"\n",
    "        # Suffix's 3 last letters\n",
    "        suf3 = token[-3:]\n",
    "        suf3 = f\"suf3={suf3}\"\n",
    "        # Suffix's 2 last letters\n",
    "        suf2 = token[-2:]\n",
    "        suf2 = f\"suf2={suf2}\"\n",
    "        # Prefix's 4 first letters\n",
    "        pre4 = token[:4]\n",
    "        pre4 = f\"pre4={pre4}\"\n",
    "        # Prefix's 3 first letters\n",
    "        pre3 = token[:3]\n",
    "        pre3 = f\"pre3={pre3}\"\n",
    "        # Prefix's 2 first letters\n",
    "        pre2 = token[:2]\n",
    "        pre2 = f\"pre2={pre2}\"\n",
    "        # Prev token\n",
    "        if i == 0:\n",
    "            prev = \"prev=_BoS_\"\n",
    "        else:\n",
    "            prev = f\"prev={token_list[i - 1][0]}\"\n",
    "        # Next token\n",
    "        if i == (len(token_list) - 1):\n",
    "            nxt = \"next=_EoS_\"\n",
    "        else:\n",
    "            nxt = f\"next={token_list[i + 1][0]}\"\n",
    "        # All token in capital letters\n",
    "        capital = f\"capital={token.isupper()}\"\n",
    "        # Begin with capital letter\n",
    "        b_capital = f\"b_capital={token[0].isupper()}\"\n",
    "        # Number of capitals in token\n",
    "        capitals = str(sum(i.isupper() for i in token))\n",
    "        # Number of digits in token\n",
    "        digits = str(sum(i.isdigit() for i in token))\n",
    "        # Number of hyphens in token\n",
    "        hyphens = str(sum('-' == i for i in token))\n",
    "        # Token length\n",
    "        leng = str(len(token))\n",
    "        # Token has Digit-Captial combination\n",
    "        dig_cap = not not match(r\"\\d+-[A-Z]+\", token)\n",
    "        dig_cap = f\"dig_cap={dig_cap}\"\n",
    "        # Feats list\n",
    "        if model == \"MaxEnt\":\n",
    "            feats = [form, pre2, pre3, pre4, suf2, suf4]\n",
    "        elif model == \"CRF\":\n",
    "            feats = [form, capital, nxt, pre2, pre3, pre4, suf2, suf4,\n",
    "                     capitals, hyphens, leng]\n",
    "        elif model == \"RandomForest\":\n",
    "            feats = [b_capital, capital, dig_cap, suf2,\n",
    "                     capitals, digits, hyphens, leng]\n",
    "        else:\n",
    "            feats = [form, b_capital, capital, dig_cap,\n",
    "                     nxt, pre2, pre3, pre4, prev, suf2, suf3, suf4,\n",
    "                     capitals, digits, hyphens, leng]\n",
    "        features.append(feats)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classifier'></a>\n",
    "## Classifier function *classifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier function takes the generated features for the data and the trained model, according to the **model** parameter and outputs the predictions given by the model. The different prediction formats of each model type are normalized into the same format and finally passed onto the [ouput_entities](#output) function.\n",
    "\n",
    "The *classifier* function makes use of an auxliary function to extract the features from the input file, attached following the main function's body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, feature_input, model_input, outputfile):\n",
    "    sentences, X_valid, Y_valid = get_sentence_features(feature_input)\n",
    "    if model == \"CRF\":\n",
    "        # CRF classifier flow\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(f\"{model_input}.crfsuite\")\n",
    "        predictions = [tagger.tag(x) for x in X_valid]\n",
    "\n",
    "    elif model == \"MaxEnt\":\n",
    "        # MaxEnt classifier flow\n",
    "        megam_features = f\"{tmp_path}/megam_valid_features.dat\"\n",
    "        megam_predictions = f\"{tmp_path}/megam_predictions.dat\"\n",
    "        system(f\"cat {feature_input} | cut -f5- | grep -v ’^$’ > \\\n",
    "            {megam_features}\")\n",
    "        system(f\"./{megam} -nc -nobias -predict {model_input}.megam multiclass\\\n",
    "            {megam_features} > {megam_predictions}\")\n",
    "        with open(megam_predictions, \"r\") as fp:\n",
    "            lines = fp.readlines()\n",
    "        pred_classes = [line.split(\"\\t\")[0] for line in lines]\n",
    "        predictions = []\n",
    "        start = 0\n",
    "        for sent in X_valid:\n",
    "            end = start + len(sent)\n",
    "            predictions.append(pred_classes[start:end])\n",
    "            start = end\n",
    "\n",
    "    elif model == \"RandomForest\":\n",
    "        with open(f\"{model_input}.randomForest\", \"rb\") as fp:\n",
    "            model, encoder = pickle.load(fp)\n",
    "        # Unlist sentences\n",
    "        x_cat = []\n",
    "        x_num = []\n",
    "        for x_sent in X_valid:\n",
    "            x_cat_sent = [f[:4] for f in x_sent]\n",
    "            x_num_sent = [f[4:] for f in x_sent]\n",
    "            x_cat.extend(x_cat_sent)\n",
    "            x_num.extend(x_num_sent)\n",
    "        # One hot encoder to turn categorical variables to binary\n",
    "        x_encoded = encoder.transform(x_cat).toarray()\n",
    "        x = np.concatenate((x_encoded, x_num), axis=1)\n",
    "        pred_classes = model.predict(x)\n",
    "        predictions = []\n",
    "        start = 0\n",
    "        for sent in X_valid:\n",
    "            end = start + len(sent)\n",
    "            predictions.append(pred_classes[start:end])\n",
    "            start = end\n",
    "\n",
    "    else:\n",
    "        print(f\"[ERROR] Model {model} not implemented\")\n",
    "        raise NotImplementedError\n",
    "    # Ouput entites for each sentence\n",
    "    with open(outputfile, \"w\") as out:\n",
    "        for sent, classes in zip(sentences, predictions):\n",
    "            id = sent[0][0]\n",
    "            tokens = [(word[1], word[2], word[3]) for word in sent if word]\n",
    "            output_entities(id, tokens, classes, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_features(input):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with open(input, \"r\") as fp:\n",
    "        lines = fp.read()\n",
    "    sentences = lines.split(\"\\n\\n\")[:-1]\n",
    "    X_feat = []\n",
    "    Y_feat = []\n",
    "    full_tokens = []\n",
    "    for sent in sentences:\n",
    "        tokens = sent.split(\"\\n\")\n",
    "        feats = [token.split(\"\\t\") for token in tokens if len(token)]\n",
    "        x = [f[5:] for f in feats if len(f)]\n",
    "        # Turn back numeric variables\n",
    "        # only for RandomForest model\n",
    "        if model == \"RandomForest\":\n",
    "            for i, token in enumerate(x):\n",
    "                val = [int(elem) if elem.isdigit() else elem for elem in token]\n",
    "                x[i] = val\n",
    "        y = [f[4] for f in feats if len(f)]\n",
    "        full_tokens.append(feats)\n",
    "        X_feat.append(x)\n",
    "        Y_feat.append(y)\n",
    "    return full_tokens, X_feat, Y_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output'></a>\n",
    "## Output generator function *output entities*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function recieves the token list  **tokens** for each sentence, identified by the **id** parameter, and their predicted classes **classes**, in the B-I-O class convention, and outputs the detected entities for the given sentence into the output file object **outf**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(id, tokens, classes, outf):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ind = 0\n",
    "    while ind < len(tokens):\n",
    "        tag = classes[ind]\n",
    "        type = tag.split(\"-\")[-1]\n",
    "        if tag == \"O\":\n",
    "            ind += 1\n",
    "            continue\n",
    "        elif \"B\" in tag:  # If Beginning of an entity\n",
    "            name, start, end = tokens[ind]\n",
    "            # Check if next token I-same_type\n",
    "            # Continue search until EoS or no-match\n",
    "            ind += 1\n",
    "            tag_nxt = classes[ind] if ind < len(tokens) else \"O\"\n",
    "            type_nxt = tag_nxt.split(\"-\")[-1]\n",
    "            while ind < len(tokens) and \"I\" in tag_nxt and type_nxt == type:\n",
    "                name_nxt, _, end_nxt = tokens[ind]\n",
    "                name = f\"{name} {name_nxt}\"\n",
    "                end = end_nxt\n",
    "                ind += 1\n",
    "                tag_nxt = classes[ind] if ind < len(tokens) else \"O\"\n",
    "                type_nxt = tag_nxt.split(\"-\")[-1]\n",
    "        else:  # I-tag\n",
    "            name, start, end = tokens[ind]\n",
    "            ind += 1\n",
    "        # Print entity and continue\n",
    "        offset = f\"{start}-{end}\"\n",
    "        txt = f\"{id}|{offset}|{name}|{type}\\n\"\n",
    "        outf.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_1'></a>\n",
    "## Evaluator output for Devel/Test sets for Goal 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Devel data-set with features indicated to obtain Goal 1\n",
    "\n",
    "With the subset of features indicated in the previous section as minimal features, we obtain a F1 average score of 0.5 with the Devel data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE RUN=1\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "776|310|0|685|296|1771|0.56|0.44|0.49\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "912|174|0|685|296|1771|0.66|0.51|0.58\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "912|0|174|685|296|1771|0.66|0.56|0.61\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "889|197|0|685|296|1771|0.64|0.5|0.56\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "495|16|0|534|44|1045|0.89|0.47|0.62\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "112|3|0|65|37|180|0.74|0.62|0.67\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "152|94|0|208|61|454|0.5|0.33|0.4\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "17|0|0|75|7|92|0.71|0.18|0.29\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.71|0.4|0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see **drug** and **brand** have F1 scores over **0.6**, meaning our rules capture well this types of entities, while for **group** and **drug_n** we have much lower scores of **0.4** and **0.29**. This was to be expected since these last types have more common multiple token entities that are not well detected through our rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Test data-set with features indicated to obtain Goal 1\n",
    "\n",
    "We know apply these minimal features to the Test data-set to see how well they generalise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE RUN=2\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "245|92|0|349|167|686|0.49|0.36|0.41\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "283|54|0|349|167|686|0.56|0.41|0.48\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "283|0|54|349|167|686|0.56|0.45|0.5\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "278|59|0|349|167|686|0.55|0.41|0.47\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "164|11|0|176|28|351|0.81|0.47|0.59\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "32|0|0|27|6|59|0.84|0.54|0.66\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "49|15|0|91|30|155|0.52|0.32|0.39\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "0|7|0|114|1|121|0|0|0\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.54|0.33|0.41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, when applying the extracted rules for recognizing and classifying our entities to the Test data-set, we realize the metrics go down below the intended threshold. This is due to the fact that our rules overfit the data in our development data-set but have a big generalization error, and thus do not apply well in the general case.\n",
    "\n",
    "In particular, we see the greatest deviation with the validation metrics in the F1 score for **drug_n**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_2'></a>\n",
    "## Evaluator output for Devel/Test sets for Goal 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Devel data-set with features indicated to obtain Goal 2\n",
    "\n",
    "In this case we add the extra features to achieve the maximum F1 score on the Devel data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE RUN=1\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|797|441|0|533|664|1771|0.42|0.45|0.43|\n",
    "\n",
    "Exact matching\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|930|308|0|533|664|1771|0.49|0.53|0.51|\n",
    "\n",
    "Partial matching\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|930|0|308|533|664|1771|0.49|0.61|0.54|\n",
    "\n",
    "type matching\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|907|331|0|533|664|1771|0.48|0.51|0.49|\n",
    "\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|485|10|0|550|42|1045|0.9|0.46|0.61|\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|111|0|0|69|26|180|0.81|0.62|0.7|\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|147|91|0|216|59|454|0.49|0.32|0.39|\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "|cor|inc|par|mis|spu|total|prec|recall|F1|\n",
    "|---|---|---|---|---|----|---|---|---|\n",
    "|54|9|0|29|70|92|0.41|0.59|0.48|\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "|P|R|F1|\n",
    "|--|--|--|\n",
    "|0.65|0.5|0.55|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous section, we know have a significantly better F1 for the previous lower types **group** and **drug_n**, with a higher score for **brand** too, while not changing the F1 score for **drug**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Devel data-set with features indicated to obtain Goal 2\n",
    "\n",
    "Like before, we see how the extra features generalise with the Test data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE RUN=2\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "255|146|0|285|401|686|0.32|0.37|0.34\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "294|107|0|285|401|686|0.37|0.43|0.4\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "294|0|107|285|401|686|0.37|0.51|0.43\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "311|90|0|285|401|686|0.39|0.45|0.42\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "164|7|0|180|28|351|0.82|0.47|0.6\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "32|0|0|27|2|59|0.94|0.54|0.69\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "49|15|0|91|30|155|0.52|0.32|0.39\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "10|35|0|76|73|121|0.08|0.08|0.08\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.59|0.35|0.44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we obtain a significantly bettwe score, we still have a high generalization error. Again, being the main discrepancy the F1 score for **drug_n** type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
