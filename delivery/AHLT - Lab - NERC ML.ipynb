{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Lab - NERC ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the deliverables for the AHLT Lab NERC Machine Learning assignment, corresponding to Goals 3 and 4.\n",
    "The notebook contains the following sections:\n",
    "\n",
    "- [Feature extractor function *extract features*](#features), with subset of features function to achieve Goals 3 and 4.\n",
    "- [Classifier function *classifier*](#classifier)\n",
    "- [Output generator function *output entities*](#output)\n",
    "- [Evaluator output for Devel/Test sets for Goal 3.](#output_1)\n",
    "- [Evaluator output for Devel/Test sets for Goal 4.](#output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features'></a>\n",
    "## Feature extractor function *extract_features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve upon the baseline entity classification throughy ruled-based entity recognition, we devise a set of features with which we train three different classifiers to detect our entities.\n",
    "\n",
    "The threee classifiers we used are:\n",
    "\n",
    "- **Conditonal Random Fields classifier** (**CRF**), through its implementation *pycrfsuite* Python package.\n",
    "- **Maximum Entropy classifier** (**MaxEnt**), through its implementation as a command line executable, details in [here](http://users.umiacs.umd.edu/~hal/megam/version0_3/)\n",
    "- **Random Forest classifier** (**RandomForest**), through its implementation in *Sklearn* Python package.\n",
    "\n",
    "For each classifier, we devise a set features that characterise the different tokens to classify them into drug entity types and using the B-I-O rules. That is we have 9 classes to classify tokens into: eight **B**(egining) or **I**(nternal) tags for each type (i.e. B-drug, I-group, etc); and a **O** tag for non-entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(token_list):\n",
    "    \"\"\"\n",
    "    Extract Features\n",
    "    Function to extract features from each token of the given token list.\n",
    "    Args:\n",
    "        - token_list: list of token strings with token words\n",
    "    Returns:\n",
    "        - features: list of list of features for each token of the given list.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for i, token_t in enumerate(token_list):\n",
    "        token, start, end = token_t\n",
    "        # Token form\n",
    "        form = f\"form={token.lower()}\"\n",
    "        # Suffix's 4 last letters\n",
    "        suf4 = token[-4:].lower()\n",
    "        suf4 = f\"suf4={suf4}\"\n",
    "        # Suffix's 3 last letters\n",
    "        suf3 = token[-3:]\n",
    "        suf3 = f\"suf3={suf3}\"\n",
    "        # Suffix's 2 last letters\n",
    "        suf2 = token[-2:]\n",
    "        suf2 = f\"suf2={suf2}\"\n",
    "        # Prefix's 4 first letters\n",
    "        pre4 = token[:4]\n",
    "        pre4 = f\"pre4={pre4}\"\n",
    "        # Prefix's 3 first letters\n",
    "        pre3 = token[:3]\n",
    "        pre3 = f\"pre3={pre3}\"\n",
    "        # Prefix's 2 first letters\n",
    "        pre2 = token[:2]\n",
    "        pre2 = f\"pre2={pre2}\"\n",
    "        # Prev token\n",
    "        if i == 0:\n",
    "            prev = \"prev=_BoS_\"\n",
    "        else:\n",
    "            prev = f\"prev={token_list[i - 1][0].lower()}\"\n",
    "        # Next token\n",
    "        if i == (len(token_list) - 1):\n",
    "            nxt = \"next=_EoS_\"\n",
    "            nxt_end = nxt\n",
    "        else:\n",
    "            nxt = f\"next={token_list[i + 1][0].lower()}\"\n",
    "            # Next token end\n",
    "            nxt_end = f\"next={token_list[i + 1][0][-3:-1]}\"\n",
    "        # All token in capital letters\n",
    "        capital_num = str(int(token.isupper()))\n",
    "        capital = f\"capital={capital_num}\"\n",
    "        # Begin with capital letter\n",
    "        b_capital_num = str(int(token[0].isupper()))\n",
    "        b_capital = f\"b_capital={b_capital_num}\"\n",
    "        # Ends s for plurals\n",
    "        ends_s_num = str(int(token.endswith('s')))\n",
    "        ends_s = f\"ends_s={ends_s_num}\"\n",
    "        # Number of has spaces in token\n",
    "        # Number of digits in token\n",
    "        digits = f\"digits={sum(i.isdigit() for i in token)}\"\n",
    "        # Number of capitals in token\n",
    "        capitals = f\"capitals={sum(i.isupper() for i in token)}\"\n",
    "        # Number of hyphens in token\n",
    "        hyphens = f\"hyphens={sum(['-' == i for i in token])}\"\n",
    "        # Number of symbols in token\n",
    "        symbols = f\"symbols={len(re.findall(r'[()+-]', token))}\"\n",
    "        # Token length\n",
    "        length = f\"length={len(token)}\"\n",
    "        # Token has Digit-Captial combination\n",
    "        dig_cap_num = str(int(bool(re.compile(\"([A-Z]+[0-9]+.*)\").match(token) or re.compile(\n",
    "            \"([0-9]+[A-Z]+.*)\").match(token))))\n",
    "        dig_cap = f\"dig_cap={dig_cap_num}\"\n",
    "        # Feats list\n",
    "        if model == \"MaxEnt\":\n",
    "            feats = [form, pre2, pre3, pre4, suf2, suf4]\n",
    "        elif model == \"CRF\":\n",
    "            # Minimum entities to reach Goal 3\n",
    "            feats = [form, capital, nxt, pre2, suf2, prev,\n",
    "                    capitals, \n",
    "                    # Entities tu reach the maximum F1\n",
    "                    pre3, pre4, suf4, dig_cap, hyphens, length\n",
    "                    ]\n",
    "        elif model == \"RandomForest\":\n",
    "            # Entities to reach Goal 3\n",
    "            feats = [suf2, pre2, nxt_end, b_capital, capital, dig_cap,\n",
    "                     capitals[-1], digits[-1], hyphens[-1], symbols[-1], length[-1]]\n",
    "        else:\n",
    "            feats = [form, b_capital, ends_s, capital, dig_cap,\n",
    "                     nxt, pre2, pre3, pre4, prev, suf2, suf3, suf4,\n",
    "                     capitals, digits, hyphens, symbols, length]\n",
    "        features.append(feats)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classifier'></a>\n",
    "## Classifier function *classifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier function takes the generated features for the data and the trained model, according to the **model** parameter and outputs the predictions given by the model. The different prediction formats of each model type are normalized into the same format and finally passed onto the [ouput_entities](#output) function.\n",
    "\n",
    "The *classifier* function makes use of an auxliary function to extract the features from the input file, attached following the main function's body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, feature_input, model_input, outputfile):\n",
    "    sentences, X_valid, Y_valid = get_sentence_features(feature_input)\n",
    "    if model == \"CRF\":\n",
    "        # CRF classifier flow\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(f\"{model_input}.crfsuite\")\n",
    "        predictions = [tagger.tag(x) for x in X_valid]\n",
    "\n",
    "    elif model == \"MaxEnt\":\n",
    "        # MaxEnt classifier flow\n",
    "        megam_features = f\"{tmp_path}/megam_valid_features.dat\"\n",
    "        megam_predictions = f\"{tmp_path}/megam_predictions.dat\"\n",
    "        system(f\"cat {feature_input} | cut -f5- | grep -v ’^$’ > \\\n",
    "            {megam_features}\")\n",
    "        system(f\"./{megam} -nc -nobias -predict {model_input}.megam multiclass\\\n",
    "            {megam_features} > {megam_predictions}\")\n",
    "        with open(megam_predictions, \"r\") as fp:\n",
    "            lines = fp.readlines()\n",
    "        pred_classes = [line.split(\"\\t\")[0] for line in lines]\n",
    "        predictions = []\n",
    "        start = 0\n",
    "        for sent in X_valid:\n",
    "            end = start + len(sent)\n",
    "            predictions.append(pred_classes[start:end])\n",
    "            start = end\n",
    "\n",
    "    elif model == \"RandomForest\":\n",
    "        with open(f\"{model_input}.randomForest\", \"rb\") as fp:\n",
    "            model, encoder = pickle.load(fp)\n",
    "        # Unlist sentences\n",
    "        x_cat = []\n",
    "        x_num = []\n",
    "        for x_sent in X_valid:\n",
    "            x_cat_sent = [f[:6] for f in x_sent]\n",
    "            x_num_sent = [f[6:] for f in x_sent]\n",
    "            x_cat.extend(x_cat_sent)\n",
    "            x_num.extend(x_num_sent)\n",
    "        # One hot encoder to turn categorical variables to binary\n",
    "        x_encoded = encoder.transform(x_cat).toarray()\n",
    "        x = np.concatenate((x_encoded, x_num), axis=1)\n",
    "        pred_classes = model.predict(x)\n",
    "        predictions = []\n",
    "        start = 0\n",
    "        for sent in X_valid:\n",
    "            end = start + len(sent)\n",
    "            predictions.append(pred_classes[start:end])\n",
    "            start = end\n",
    "\n",
    "    else:\n",
    "        print(f\"[ERROR] Model {model} not implemented\")\n",
    "        raise NotImplementedError\n",
    "    # Ouput entites for each sentence\n",
    "    with open(outputfile, \"w\") as out:\n",
    "        for sent, classes in zip(sentences, predictions):\n",
    "            id = sent[0][0]\n",
    "            tokens = [(word[1], word[2], word[3]) for word in sent if word]\n",
    "            output_entities(id, tokens, classes, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_features(input):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with open(input, \"r\") as fp:\n",
    "        lines = fp.read()\n",
    "    sentences = lines.split(\"\\n\\n\")[:-1]\n",
    "    X_feat = []\n",
    "    Y_feat = []\n",
    "    full_tokens = []\n",
    "    for sent in sentences:\n",
    "        tokens = sent.split(\"\\n\")\n",
    "        feats = [token.split(\"\\t\") for token in tokens if len(token)]\n",
    "        x = [f[5:] for f in feats if len(f)]\n",
    "        # Turn back numeric variables\n",
    "        # only for RandomForest model\n",
    "        if model == \"RandomForest\":\n",
    "            for i, token in enumerate(x):\n",
    "                val = [int(elem) if elem.isdigit() else elem for elem in token]\n",
    "                x[i] = val\n",
    "        y = [f[4] for f in feats if len(f)]\n",
    "        full_tokens.append(feats)\n",
    "        X_feat.append(x)\n",
    "        Y_feat.append(y)\n",
    "    return full_tokens, X_feat, Y_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output'></a>\n",
    "## Output generator function *output entities*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function recieves the token list  **tokens** for each sentence, identified by the **id** parameter, and their predicted classes **classes**, in the B-I-O class convention, and outputs the detected entities for the given sentence into the output file object **outf**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(id, tokens, classes, outf):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ind = 0\n",
    "    while ind < len(tokens):\n",
    "        tag = classes[ind]\n",
    "        type = tag.split(\"-\")[-1]\n",
    "        if tag == \"O\":\n",
    "            ind += 1\n",
    "            continue\n",
    "        elif \"B\" in tag:  # If Beginning of an entity\n",
    "            name, start, end = tokens[ind]\n",
    "            # Check if next token I-same_type\n",
    "            # Continue search until EoS or no-match\n",
    "            ind += 1\n",
    "            tag_nxt = classes[ind] if ind < len(tokens) else \"O\"\n",
    "            type_nxt = tag_nxt.split(\"-\")[-1]\n",
    "            while ind < len(tokens) and \"I\" in tag_nxt and type_nxt == type:\n",
    "                name_nxt, _, end_nxt = tokens[ind]\n",
    "                name = f\"{name} {name_nxt}\"\n",
    "                end = end_nxt\n",
    "                ind += 1\n",
    "                tag_nxt = classes[ind] if ind < len(tokens) else \"O\"\n",
    "                type_nxt = tag_nxt.split(\"-\")[-1]\n",
    "        else:  # I-tag\n",
    "            name, start, end = tokens[ind]\n",
    "            ind += 1\n",
    "        # Print entity and continue\n",
    "        offset = f\"{start}-{end}\"\n",
    "        txt = f\"{id}|{offset}|{name}|{type}\\n\"\n",
    "        outf.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_1'></a>\n",
    "## Evaluator output for CRF and Devel/Test sets for Goal 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output for the Devel data-set with features indicated to obtain Goal 3\n",
    "We have implemented de CRF model on de NLP model. With the subset of features indicated in the previous section as minimal features to obtain a F1 average score  over 0.6 with the Devel data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: BASELINE RUN=1\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1277|179|0|315|75|1771|0.83|0.72|0.77\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1342|114|0|315|75|1771|0.88|0.76|0.81\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1342|0|114|315|75|1771|0.88|0.79|0.83\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1370|86|0|315|75|1771|0.89|0.77|0.83\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "853|32|0|160|66|1045|0.9|0.82|0.85\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "115|0|0|65|5|180|0.96|0.64|0.77\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "304|66|0|84|20|454|0.78|0.67|0.72\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "5|0|0|87|1|92|0.83|0.05|0.1\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.87|0.54|0.61\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see **drug** and **brand** have F1 scores over **0.7**, meaning our rules capture well this types of entities, while for **group** and **drug_n** we have much lower scores of **0.72** and **0.1**. This was to be expected since these last types have more common multiple token entities that are not well detected through our rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for CRF and the Devel data-set with features indicated to obtain Goal 3\n",
    "\n",
    "We have implemented the CRF model. Then, with the subset of features indicated in the previous section as  features to obtain the maximum F1 average score with the Devel data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: CRF RUN=1\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1343|180|0|248|63|1771|0.85|0.76|0.8\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1415|108|0|248|63|1771|0.89|0.8|0.84\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1415|0|108|248|63|1771|0.89|0.83|0.86\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1440|83|0|248|63|1771|0.91|0.81|0.86\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "886|43|0|116|65|1045|0.89|0.85|0.87\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "112|3|0|65|2|180|0.96|0.62|0.75\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "329|51|0|74|17|454|0.83|0.72|0.77\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "16|1|0|75|0|92|0.94|0.17|0.29\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.9|0.59|0.67\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the only remaining type of giving us bad results is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for Random Forest and the Test data-set with features indicated to obtain Goal 3\n",
    "\n",
    "Now we test our model using Random Forest with the Test data set. The best results we have obtained using the commented features are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: CRF \n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1231|260|0|280|155|1771|0.75|0.7|0.72\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1294|197|0|280|155|1771|0.79|0.73|0.76\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1294|0|197|280|155|1771|0.79|0.79|0.79\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1407|84|0|280|155|1771|0.85|0.79|0.82\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "837|33|0|175|80|1045|0.88|0.8|0.84\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "112|0|0|68|2|180|0.98|0.62|0.76\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "270|143|0|41|46|454|0.59|0.59|0.59\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "12|0|0|80|1|92|0.92|0.13|0.23\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.84|0.54|0.61\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
