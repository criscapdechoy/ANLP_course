{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Lab - NERC ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the deliverables for the AHLT Lab NERC Machine Learning assignment, corresponding to Goals 3 and 4.\n",
    "The notebook contains the following sections:\n",
    "\n",
    "- [Feature extractor function *extract features*](#features), with subset of features function to achieve Goals 3 and 4.\n",
    "- [Classifier function *classifier*](#classifier)\n",
    "- [Output generator function *output entities*](#output)\n",
    "- [Evaluator output for Devel/Test sets for Goal 3.](#output_1)\n",
    "- [Evaluator output for Devel/Test sets for Goal 4.](#output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features'></a>\n",
    "## Feature extractor function *extract_features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve upon the baseline entity classification throughy ruled-based entity recognition, we devise a set of features with which we train three different classifiers to detect our entities.\n",
    "\n",
    "The threee classifiers we used are:\n",
    "\n",
    "- **Conditonal Random Fields classifier** (**CRF**), through its implementation *pycrfsuite* Python package.\n",
    "- **Maximum Entropy classifier** (**MaxEnt**), through its implementation as a command line executable, details in [here](http://users.umiacs.umd.edu/~hal/megam/version0_3/)\n",
    "- **Random Forest classifier** (**RandomForest**), through its implementation in *Sklearn* Python package.\n",
    "\n",
    "For each classifier, we devise a set features that characterise the different tokens to classify them into drug entity types and using the B-I-O rules. That is we have 9 classes to classify tokens into: eight **B**(egining) or **I**(nternal) tags for each type (i.e. B-drug, I-group, etc); and a **O** tag for non-entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(token_list):\n",
    "    \"\"\"\n",
    "    Extract Features\n",
    "    Function to extract features from each token of the given token list.\n",
    "    Args:\n",
    "        - token_list: list of token strings with token words\n",
    "    Returns:\n",
    "        - features: list of list of features for each token of the given list.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for i, token_t in enumerate(token_list):\n",
    "        token, start, end = token_t\n",
    "        # Token form\n",
    "        form = f\"form={token.lower()}\"\n",
    "        # Suffix's 4 last letters\n",
    "        suf4 = f\"suf4={token[-4:].lower()}\"\n",
    "        # Suffix's 3 last letters\n",
    "        suf3 = f\"suf3={token[-3:]}\"\n",
    "        # Suffix's 2 last letters\n",
    "        suf2 = f\"suf2={token[-2:]}\"\n",
    "        # Prefix's 4 first letters\n",
    "        pre4 = f\"pre4={token[:4]}\"\n",
    "        # Prefix's 3 first letters\n",
    "        pre3 = f\"pre3={token[:3]}\"\n",
    "        # Prefix's 2 first letters\n",
    "        pre2 = f\"pre2={token[:2]}\"\n",
    "        # Prev token\n",
    "        if i == 0:\n",
    "            prev = \"prev=_BoS_\"\n",
    "        else:\n",
    "            prev = f\"prev={token_list[i - 1][0].lower()}\"\n",
    "        # Next token\n",
    "        if i == (len(token_list) - 1):\n",
    "            nxt = \"next=_EoS_\"\n",
    "            nxt_end = nxt\n",
    "        else:\n",
    "            nxt = f\"next={token_list[i + 1][0].lower()}\"\n",
    "            # Next token end\n",
    "            nxt_end = f\"next={token_list[i + 1][0][-3:-1]}\"\n",
    "        # All token in capital letters\n",
    "        capital_num = str(int(token.isupper()))\n",
    "        capital = f\"capital={capital_num}\"\n",
    "        # Begin with capital letter\n",
    "        b_capital_num = str(int(token[0].isupper()))\n",
    "        b_capital = f\"b_capital={b_capital_num}\"\n",
    "        # Number of digits in token\n",
    "        digits = f\"digits={sum(i.isdigit() for i in token)}\"\n",
    "        # Number of capitals in token\n",
    "        capitals = f\"capitals={sum(i.isupper() for i in token)}\"\n",
    "        # Number of hyphens in token\n",
    "        hyphens = f\"hyphens={sum(['-' == i for i in token])}\"\n",
    "        # Number of symbols in token\n",
    "        symbols = f\"symbols={len(re.findall(r'[()+-]', token))}\"\n",
    "        # Token length\n",
    "        length = f\"length={len(token)}\"\n",
    "        # Token has Digit-Captial combination\n",
    "        dig_cap_num = str(int(bool(re.compile(\"([A-Z]+[0-9]+.*)\").match(token) or re.compile(\n",
    "            \"([0-9]+[A-Z]+.*)\").match(token))))\n",
    "        dig_cap = f\"dig_cap={dig_cap_num}\"\n",
    "        # Feats list\n",
    "        if model == \"MaxEnt\":\n",
    "            # Entities to reach Goal 3\n",
    "            feats = [form, pre2, pre3, pre4, suf2, suf4]\n",
    "        elif model == \"CRF\":\n",
    "            # Features to reach Goal 3\n",
    "            feats = [form, capital, nxt, pre2, suf2, prev,\n",
    "                    capitals, \n",
    "                    # Features to reach Goal 4\n",
    "                    pre3, pre4, suf4, dig_cap, hyphens, length\n",
    "                    ]\n",
    "        elif model == \"RandomForest\":\n",
    "            # Entities to reach Goal 3\n",
    "            feats = [suf2, pre2, nxt_end, b_capital, capital, dig_cap,\n",
    "                     capitals[-1], digits[-1], hyphens[-1], symbols[-1], length[-1]]\n",
    "        else:\n",
    "            print(f\"[ERROR] Model {model} not implemented\")\n",
    "            raise NotImplementedError\n",
    "        features.append(feats)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classifier'></a>\n",
    "## Classifier function *classifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier function takes the generated features for the data and the trained model, according to the **model** parameter and outputs the predictions given by the model. The different prediction formats of each model type are normalized into the same format and finally passed onto the [ouput_entities](#output) function.\n",
    "\n",
    "The *classifier* function makes use of an auxliary function to extract the features from the input file, attached following the main function's body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, feature_input, model_input, outputfile):\n",
    "    sentences, X_valid, Y_valid = get_sentence_features(feature_input)\n",
    "    if model == \"CRF\":\n",
    "        # CRF classifier flow\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(f\"{model_input}.crfsuite\")\n",
    "        predictions = [tagger.tag(x) for x in X_valid]\n",
    "\n",
    "    elif model == \"MaxEnt\":\n",
    "        # MaxEnt classifier flow\n",
    "        megam_features = f\"{tmp_path}/megam_valid_features.dat\"\n",
    "        megam_predictions = f\"{tmp_path}/megam_predictions.dat\"\n",
    "        system(f\"cat {feature_input} | cut -f5- | grep -v ’^$’ > \\\n",
    "            {megam_features}\")\n",
    "        system(f\"./{megam} -nc -nobias -predict {model_input}.megam multiclass\\\n",
    "            {megam_features} > {megam_predictions}\")\n",
    "        with open(megam_predictions, \"r\") as fp:\n",
    "            lines = fp.readlines()\n",
    "        pred_classes = [line.split(\"\\t\")[0] for line in lines]\n",
    "        predictions = []\n",
    "        start = 0\n",
    "        for sent in X_valid:\n",
    "            end = start + len(sent)\n",
    "            predictions.append(pred_classes[start:end])\n",
    "            start = end\n",
    "\n",
    "    elif model == \"RandomForest\":\n",
    "        with open(f\"{model_input}.randomForest\", \"rb\") as fp:\n",
    "            model, encoder = pickle.load(fp)\n",
    "        # Unlist sentences\n",
    "        x_cat = []\n",
    "        x_num = []\n",
    "        for x_sent in X_valid:\n",
    "            x_cat_sent = [f[:6] for f in x_sent]\n",
    "            x_num_sent = [f[6:] for f in x_sent]\n",
    "            x_cat.extend(x_cat_sent)\n",
    "            x_num.extend(x_num_sent)\n",
    "        # One hot encoder to turn categorical variables to binary\n",
    "        x_encoded = encoder.transform(x_cat).toarray()\n",
    "        x = np.concatenate((x_encoded, x_num), axis=1)\n",
    "        pred_classes = model.predict(x)\n",
    "        predictions = []\n",
    "        start = 0\n",
    "        for sent in X_valid:\n",
    "            end = start + len(sent)\n",
    "            predictions.append(pred_classes[start:end])\n",
    "            start = end\n",
    "\n",
    "    else:\n",
    "        print(f\"[ERROR] Model {model} not implemented\")\n",
    "        raise NotImplementedError\n",
    "    # Ouput entites for each sentence\n",
    "    with open(outputfile, \"w\") as out:\n",
    "        for sent, classes in zip(sentences, predictions):\n",
    "            id = sent[0][0]\n",
    "            tokens = [(word[1], word[2], word[3]) for word in sent if word]\n",
    "            output_entities(id, tokens, classes, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_features(input):\n",
    "    with open(input, \"r\") as fp:\n",
    "        lines = fp.read()\n",
    "    sentences = lines.split(\"\\n\\n\")[:-1]\n",
    "    X_feat = []\n",
    "    Y_feat = []\n",
    "    full_tokens = []\n",
    "    for sent in sentences:\n",
    "        tokens = sent.split(\"\\n\")\n",
    "        feats = [token.split(\"\\t\") for token in tokens if len(token)]\n",
    "        x = [f[5:] for f in feats if len(f)]\n",
    "        # Turn back numeric variables\n",
    "        # only for RandomForest model\n",
    "        if model == \"RandomForest\":\n",
    "            for i, token in enumerate(x):\n",
    "                val = [int(elem) if elem.isdigit() else elem for elem in token]\n",
    "                x[i] = val\n",
    "        y = [f[4] for f in feats if len(f)]\n",
    "        full_tokens.append(feats)\n",
    "        X_feat.append(x)\n",
    "        Y_feat.append(y)\n",
    "    return full_tokens, X_feat, Y_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output'></a>\n",
    "## Output generator function *output entities*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function recieves the token list  **tokens** for each sentence, identified by the **id** parameter, and their predicted classes **classes**, in the B-I-O class convention, and outputs the detected entities for the given sentence into the output file object **outf**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(id, tokens, classes, outf):\n",
    "    ind = 0\n",
    "    while ind < len(tokens):\n",
    "        tag = classes[ind]\n",
    "        type = tag.split(\"-\")[-1]\n",
    "        if tag == \"O\":\n",
    "            ind += 1\n",
    "            continue\n",
    "        elif \"B\" in tag:  # If Beginning of an entity\n",
    "            name, start, end = tokens[ind]\n",
    "            # Check if next token I-same_type\n",
    "            # Continue search until EoS or no-match\n",
    "            ind += 1\n",
    "            tag_nxt = classes[ind] if ind < len(tokens) else \"O\"\n",
    "            type_nxt = tag_nxt.split(\"-\")[-1]\n",
    "            while ind < len(tokens) and \"I\" in tag_nxt and type_nxt == type:\n",
    "                name_nxt, _, end_nxt = tokens[ind]\n",
    "                name = f\"{name} {name_nxt}\"\n",
    "                end = end_nxt\n",
    "                ind += 1\n",
    "                tag_nxt = classes[ind] if ind < len(tokens) else \"O\"\n",
    "                type_nxt = tag_nxt.split(\"-\")[-1]\n",
    "        else:  # I-tag\n",
    "            name, start, end = tokens[ind]\n",
    "            ind += 1\n",
    "        # Print entity and continue\n",
    "        offset = f\"{start}-{end}\"\n",
    "        txt = f\"{id}|{offset}|{name}|{type}\\n\"\n",
    "        outf.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_1'></a>\n",
    "## Evaluator output for CRF and Devel/Test sets for Goal 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Devel data-set with features indicated to obtain Goal 3\n",
    "We present the output of all the model, where the CRF model, the best, is given with the minimal features to obtain the F1 score of 0.6 corresponding to Goal 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: ML \n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|1277|179|0|315|75|1771|0.83|0.72|0.77\n",
    "**MaxEnt**|993|374|0|404|162|1771|0.65|0.56|0.6\n",
    "**RandomForest**|1231|260|0|280|155|1771|0.75|0.7|0.72\n",
    "\n",
    "Exact matching\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|1342|114|0|315|75|1771|0.88|0.76|0.81\n",
    "**MaxEnt**|1077|290|0|404|162|1771|0.7|0.61|0.65\n",
    "**RandomForest**|1294|197|0|280|155|1771|0.79|0.73|0.76\n",
    "\n",
    "Partial matching\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|1342|0|114|315|75|1771|0.88|0.79|0.83\n",
    "**MaxEnt**|1077|0|290|404|162|1771|0.7|0.69|0.7\n",
    "**RandomForest**|1294|0|197|280|155|1771|0.79|0.79|0.79\n",
    "\n",
    "type matching\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|1370|86|0|315|75|1771|0.89|0.77|0.83\n",
    "**MaxEnt**|1223|144|0|404|162|1771|0.8|0.69|0.74\n",
    "**RandomForest**|1407|84|0|280|155|1771|0.85|0.79|0.82\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|853|32|0|160|66|1045|0.9|0.82|0.85\n",
    "**MaxEnt**|731|94|0|220|139|1045|0.76|0.7|0.73\n",
    "**RandomForest**|837|33|0|175|80|1045|0.88|0.8|0.84\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|115|0|0|65|5|180|0.96|0.64|0.77\n",
    "**MaxEnt**|98|2|0|80|1|180|0.97|0.54|0.7\n",
    "**RandomForest**|112|0|0|68|2|180|0.98|0.62|0.76\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|304|66|0|84|20|454|0.78|0.67|0.72\n",
    "**MaxEnt**|144|134|0|176|41|454|0.45|0.32|0.37\n",
    "**RandomForest**|270|143|0|41|46|454|0.59|0.59|0.59\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|5|0|0|87|1|92|0.83|0.05|0.1\n",
    "**MaxEnt**|20|0|0|72|1|92|0.95|0.22|0.35\n",
    "**RandomForest**|12|0|0|80|1|92|0.92|0.13|0.23\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "*Model*|P|R|F1\n",
    "-------|---|---|---\n",
    "**CRF**|0.87|0.54|0.61\n",
    "**MaxEnt**|0.78|0.44|0.54\n",
    "**RandomForest**|0.84|0.54|0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see **drug** and **brand** have high F1 scores over **0.7** for all three models, while only **CRF** and **RandomForest** models have high score for **group**, meaning our features capture well this types of entities.\n",
    "\n",
    "In contrast, the **CRF** model has much lower scores of **0.1** for **drug_n** entities, while **RandomForest** and **MaxEnt** models have, increasingly better scores. In spite of this, all models have 0 incorrect predictions on this type.\n",
    "\n",
    "This result indicate us that our features do not characterise well these type *drug_n* entities, while they characterise pretty good other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Test data-set with features indicated to obtain Goal 3\n",
    "\n",
    "We test the generalisation of the previously presented models with the Test data-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: ML RUN=2\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|372|113|0|201|47|686|0.7|0.54|0.61\n",
    "**MaxEnt**|326|191|0|169|132|686|0.5|0.48|0.49\n",
    "**RandomForest**|372|149|0|165|105|686|0.59|0.54|0.57\n",
    "\n",
    "Exact matching\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|431|54|0|201|47|686|0.81|0.63|0.71\n",
    "**MaxEnt**|385|132|0|169|132|686|0.59|0.56|0.58\n",
    "**RandomForest**|430|91|0|165|105|686|0.69|0.63|0.66\n",
    "\n",
    "Partial matching\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|431|0|54|201|47|686|0.81|0.67|0.73\n",
    "**MaxEnt**|385|0|132|169|132|686|0.59|0.66|0.62\n",
    "**RandomForest**|1430|0|91|165|105|686|0.69|0.69|0.69\n",
    "\n",
    "type matching\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|404|81|0|201|47|686|0.76|0.59|0.66\n",
    "**MaxEnt**|409|108|0|169|132|686|0.63|0.6|0.61\n",
    "**RandomForest**|435|86|0|165|105|686|0.69|0.63|0.66\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|260|20|0|71|46|351|0.8|0.74|0.77\n",
    "**MaxEnt**|238|48|0|65|76|351|0.66|0.68|0.67\n",
    "**RandomForest**|250|36|0|65|57|351|0.73|0.71|0.72\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|28|0|0|31|0|59|1|0.47|0.64\n",
    "**MaxEnt**|21|0|0|38|0|59|1|0.36|0.53\n",
    "**RandomForest**|26|0|0|33|0|59|1|0.44|0.61\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|84|13|0|58|12|155|0.77|0.54|0.64\n",
    "**MaxEnt**|62|38|0|55|18|155|0.53|0.4|0.45\n",
    "**RandomForest**|91|27|0|37|27|155|0.63|0.59|0.61\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "*Model*|cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "-------|---|---|---|---|---|----|---|---|---\n",
    "**CRF**|0|0|0|121|0|121|0|0|0\n",
    "**MaxEnt**|5|0|0|116|2|121|0.71|0.04|0.08\n",
    "**RandomForest**|5|0|0|116|0|121|1|0.04|0.08\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "*Model*|P|R|F1\n",
    "-------|---|---|---\n",
    "**CRF**|0.64|0.44|0.51\n",
    "**MaxEnt**|0.72|0.37|0.43\n",
    "**RandomForest**|0.84|0.45|0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for the Test set we obtain the same trend as for the Devel set, where the types *drug*, *brand*, and *group* are better classified than the *drug_n*, better by the **MaxEnt** and **RandomForest** models.\n",
    "\n",
    "Nevertheless, the best overall model on the test data-set is the **CRF** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_2'></a>\n",
    "## Evaluator output for CRF and Devel/Test sets for Goal 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Devel data-set with features indicated to obtain Goal 4\n",
    "\n",
    "We improve the previous **CRF** model by adding some extra features, marked as *features to obtain the maximum F1* in the function code, and achieve the maximum F1 score for the Devel data-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: CRF\n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1415|108|0|248|63|1771|0.89|0.8|0.84\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1415|0|108|248|63|1771|0.89|0.83|0.86\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "1440|83|0|248|63|1771|0.91|0.81|0.86\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "442|71|0|173|35|686|0.81|0.64|0.72\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "886|43|0|116|65|1045|0.89|0.85|0.87\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "112|3|0|65|2|180|0.96|0.62|0.75\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "329|51|0|74|17|454|0.83|0.72|0.77\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "16|1|0|75|0|92|0.94|0.17|0.29\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.9|0.59|0.67\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see **drug** has a high score of 0.87, **brand** and **group** have F1 scores over 0.75, meaning our rules capture well this types of entities, while for **drug_n** we have much lower score of 0.29. Still, **drug_n** is having a too low score. The features are not capturing its pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Test data-set with features indicated to obtain Goal 4\n",
    "\n",
    "Now we test our model using CRF with the Test data set. The best results we have obtained using the commented features are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: CRF \n",
    "\n",
    "Strict matching (boundaries + type)\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "397|116|0|173|35|686|0.72|0.58|0.64\n",
    "\n",
    "Exact matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "441|72|0|173|35|686|0.8|0.64|0.71\n",
    "\n",
    "Partial matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "441|0|72|173|35|686|0.8|0.7|0.75\n",
    "\n",
    "type matching\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "442|71|0|173|35|686|0.81|0.64|0.72\n",
    "\n",
    "#### SCORES FOR ENTITY TYPE\n",
    "\n",
    "Exact matching on drug\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "265|28|0|58|35|351|0.81|0.75|0.78\n",
    "\n",
    "Exact matching on brand\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "27|0|0|32|0|59|1|0.46|0.63\n",
    "\n",
    "Exact matching on group\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "102|16|0|37|9|155|0.8|0.66|0.72\n",
    "\n",
    "Exact matching on drug_n\n",
    "\n",
    "cor|inc|par|mis|spu|total|prec|recall|F1\n",
    "---|---|---|---|---|----|---|---|---\n",
    "3|3|0|115|0|121|0.5|0.02|0.05\n",
    "\n",
    "#### MACRO-AVERAGE MEASURES:\n",
    "\n",
    "P|R|F1\n",
    "---|---|---\n",
    "0.78|0.47|0.54\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see **drug** and **group** have F1 scores over 0.7, meaning our rules capture well this types of entities, while for **brand** and, again, **drug_n** we have much lower scores of 0.63 and 0.05. These values inform us that we need to improve the **drug_n** recognition to improve our model. It is the entity with always bad results which makes the average F1 score always fall down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
