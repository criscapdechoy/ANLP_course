{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Lab - DDI ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricard Monge and Cristina Capdevila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the deliverables for the AHLT Lab DDI Machine Learning assignment.\n",
    "The notebook contains the following sections:\n",
    "@TOREVISE\n",
    "- [Feature extractor function *extract features*](#features), with subset of features function to achieve Goals 3 and 4.\n",
    "- [Classifier function *classifier*](#classifier)\n",
    "- [Output generator function *output entities*](#output)\n",
    "- [Evaluator output for Devel/Test sets for Goal 3.](#output_1)\n",
    "- [Evaluator output for Devel/Test sets for Goal 4.](#output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features'></a>\n",
    "## Feature extractor function *extract_features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve upon the baseline DDI classification we devise a set of features with which we train four different classifiers to detect the interactions.\n",
    "\n",
    "The fours classifiers we have tested are:\n",
    "- **Maximum Entropy classifier** (**MaxEnt**), through its implementation as a command line executable, details in [here](http://users.umiacs.umd.edu/~hal/megam/version0_3/)\n",
    "- **Multi-layer Perceptron Classifier** (**MLP**), through its implementation in *Sklearn* Python package. We have used a unique hidden layer with size of 45.\n",
    "- **Support Vector Classification** (**SVC**), through its implementation in *Sklearn* Python package.\n",
    "- **Logistic Regression** (**LR**), through its implementation in *Sklearn* Python package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(token_list):\n",
    "    \"\"\"\n",
    "    Extract Features\n",
    "    Function to extract features from each token of the given token list.\n",
    "    Args:\n",
    "        - token_list: list of token strings with token words\n",
    "    Returns:\n",
    "        - features: list of list of features for each token of the given list.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for i, token_t in enumerate(token_list):\n",
    "        token, start, end = token_t\n",
    "        # Token form\n",
    "        form = f\"form={token.lower()}\"\n",
    "        # Suffix's 4 last letters\n",
    "        suf4 = f\"suf4={token[-4:].lower()}\"\n",
    "        # Suffix's 3 last letters\n",
    "        suf3 = f\"suf3={token[-3:]}\"\n",
    "        # Suffix's 2 last letters\n",
    "        suf2 = f\"suf2={token[-2:]}\"\n",
    "        # Prefix's 4 first letters\n",
    "        pre4 = f\"pre4={token[:4]}\"\n",
    "        # Prefix's 3 first letters\n",
    "        pre3 = f\"pre3={token[:3]}\"\n",
    "        # Prefix's 2 first letters\n",
    "        pre2 = f\"pre2={token[:2]}\"\n",
    "        # Prev token\n",
    "        if i == 0:\n",
    "            prev = \"prev=_BoS_\"\n",
    "        else:\n",
    "            prev = f\"prev={token_list[i - 1][0].lower()}\"\n",
    "        # Next token\n",
    "        if i == (len(token_list) - 1):\n",
    "            nxt = \"next=_EoS_\"\n",
    "            nxt_end = nxt\n",
    "        else:\n",
    "            nxt = f\"next={token_list[i + 1][0].lower()}\"\n",
    "            # Next token end\n",
    "            nxt_end = f\"next={token_list[i + 1][0][-3:-1]}\"\n",
    "        # All token in capital letters\n",
    "        capital_num = str(int(token.isupper()))\n",
    "        capital = f\"capital={capital_num}\"\n",
    "        # Begin with capital letter\n",
    "        b_capital_num = str(int(token[0].isupper()))\n",
    "        b_capital = f\"b_capital={b_capital_num}\"\n",
    "        # Number of digits in token\n",
    "        digits = f\"digits={sum(i.isdigit() for i in token)}\"\n",
    "        # Number of capitals in token\n",
    "        capitals = f\"capitals={sum(i.isupper() for i in token)}\"\n",
    "        # Number of hyphens in token\n",
    "        hyphens = f\"hyphens={sum(['-' == i for i in token])}\"\n",
    "        # Number of symbols in token\n",
    "        symbols = f\"symbols={len(re.findall(r'[()+-]', token))}\"\n",
    "        # Token length\n",
    "        length = f\"length={len(token)}\"\n",
    "        # Token has Digit-Captial combination\n",
    "        dig_cap_num = str(int(bool(re.compile(\"([A-Z]+[0-9]+.*)\").match(token) or re.compile(\n",
    "            \"([0-9]+[A-Z]+.*)\").match(token))))\n",
    "        dig_cap = f\"dig_cap={dig_cap_num}\"\n",
    "        # Feats list\n",
    "        if model == \"MaxEnt\":\n",
    "            # Entities to reach Goal 3\n",
    "            feats = [form, pre2, pre3, pre4, suf2, suf4]\n",
    "        elif model == \"CRF\":\n",
    "            # Features to reach Goal 3\n",
    "            feats = [form, capital, nxt, pre2, suf2, prev,\n",
    "                    capitals, \n",
    "                    # Features to reach Goal 4\n",
    "                    pre3, pre4, suf4, dig_cap, hyphens, length\n",
    "                    ]\n",
    "        elif model == \"RandomForest\":\n",
    "            # Entities to reach Goal 3\n",
    "            feats = [suf2, pre2, nxt_end, b_capital, capital, dig_cap,\n",
    "                     capitals[-1], digits[-1], hyphens[-1], symbols[-1], length[-1]]\n",
    "        else:\n",
    "            print(f\"[ERROR] Model {model} not implemented\")\n",
    "            raise NotImplementedError\n",
    "        features.append(feats)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classifier'></a>\n",
    "## Classifier function *classifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier function takes the generated features for the data and the trained model, according to the **model** parameter and outputs the predictions given by the model. The different prediction formats of each model type are normalized into the same format and finally passed onto the [ouput_entities](#output) function.\n",
    "\n",
    "The *classifier* function makes use of an auxliary function to extract the features from the input file, attached following the main function's body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, feature_input, model_input, outputfile):\n",
    "    sentences, X_valid, Y_valid = get_sentence_features(feature_input)\n",
    "    if model == \"CRF\":\n",
    "        # CRF classifier flow\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(f\"{model_input}.crfsuite\")\n",
    "        predictions = [tagger.tag(x) for x in X_valid]\n",
    "\n",
    "    elif model == \"MaxEnt\":\n",
    "        # MaxEnt classifier flow\n",
    "        megam_features = f\"{tmp_path}/megam_valid_features.dat\"\n",
    "        megam_predictions = f\"{tmp_path}/megam_predictions.dat\"\n",
    "        system(f\"cat {feature_input} | cut -f5- | grep -v ’^$’ > \\\n",
    "            {megam_features}\")\n",
    "        system(f\"./{megam} -nc -nobias -predict {model_input}.megam multiclass\\\n",
    "            {megam_features} > {megam_predictions}\")\n",
    "        with open(megam_predictions, \"r\") as fp:\n",
    "            lines = fp.readlines()\n",
    "        pred_classes = [line.split(\"\\t\")[0] for line in lines]\n",
    "        predictions = []\n",
    "        start = 0\n",
    "        for sent in X_valid:\n",
    "            end = start + len(sent)\n",
    "            predictions.append(pred_classes[start:end])\n",
    "            start = end\n",
    "\n",
    "    elif model == \"RandomForest\":\n",
    "        with open(f\"{model_input}.randomForest\", \"rb\") as fp:\n",
    "            model, encoder = pickle.load(fp)\n",
    "        # Unlist sentences\n",
    "        x_cat = []\n",
    "        x_num = []\n",
    "        for x_sent in X_valid:\n",
    "            x_cat_sent = [f[:6] for f in x_sent]\n",
    "            x_num_sent = [f[6:] for f in x_sent]\n",
    "            x_cat.extend(x_cat_sent)\n",
    "            x_num.extend(x_num_sent)\n",
    "        # One hot encoder to turn categorical variables to binary\n",
    "        x_encoded = encoder.transform(x_cat).toarray()\n",
    "        x = np.concatenate((x_encoded, x_num), axis=1)\n",
    "        pred_classes = model.predict(x)\n",
    "        predictions = []\n",
    "        start = 0\n",
    "        for sent in X_valid:\n",
    "            end = start + len(sent)\n",
    "            predictions.append(pred_classes[start:end])\n",
    "            start = end\n",
    "\n",
    "    else:\n",
    "        print(f\"[ERROR] Model {model} not implemented\")\n",
    "        raise NotImplementedError\n",
    "    # Ouput entites for each sentence\n",
    "    with open(outputfile, \"w\") as out:\n",
    "        for sent, classes in zip(sentences, predictions):\n",
    "            id = sent[0][0]\n",
    "            tokens = [(word[1], word[2], word[3]) for word in sent if word]\n",
    "            output_entities(id, tokens, classes, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_features(input):\n",
    "    with open(input, \"r\") as fp:\n",
    "        lines = fp.read()\n",
    "    sentences = lines.split(\"\\n\\n\")[:-1]\n",
    "    X_feat = []\n",
    "    Y_feat = []\n",
    "    full_tokens = []\n",
    "    for sent in sentences:\n",
    "        tokens = sent.split(\"\\n\")\n",
    "        feats = [token.split(\"\\t\") for token in tokens if len(token)]\n",
    "        x = [f[5:] for f in feats if len(f)]\n",
    "        # Turn back numeric variables\n",
    "        # only for RandomForest model\n",
    "        if model == \"RandomForest\":\n",
    "            for i, token in enumerate(x):\n",
    "                val = [int(elem) if elem.isdigit() else elem for elem in token]\n",
    "                x[i] = val\n",
    "        y = [f[4] for f in feats if len(f)]\n",
    "        full_tokens.append(feats)\n",
    "        X_feat.append(x)\n",
    "        Y_feat.append(y)\n",
    "    return full_tokens, X_feat, Y_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output'></a>\n",
    "## Output generator function *output entities*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function recieves the token list  **tokens** for each sentence, identified by the **id** parameter, and their predicted classes **classes**, in the B-I-O class convention, and outputs the detected entities for the given sentence into the output file object **outf**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(id, tokens, classes, outf):\n",
    "    ind = 0\n",
    "    while ind < len(tokens):\n",
    "        tag = classes[ind]\n",
    "        type = tag.split(\"-\")[-1]\n",
    "        if tag == \"O\":\n",
    "            ind += 1\n",
    "            continue\n",
    "        elif \"B\" in tag:  # If Beginning of an entity\n",
    "            name, start, end = tokens[ind]\n",
    "            # Check if next token I-same_type\n",
    "            # Continue search until EoS or no-match\n",
    "            ind += 1\n",
    "            tag_nxt = classes[ind] if ind < len(tokens) else \"O\"\n",
    "            type_nxt = tag_nxt.split(\"-\")[-1]\n",
    "            while ind < len(tokens) and \"I\" in tag_nxt and type_nxt == type:\n",
    "                name_nxt, _, end_nxt = tokens[ind]\n",
    "                name = f\"{name} {name_nxt}\"\n",
    "                end = end_nxt\n",
    "                ind += 1\n",
    "                tag_nxt = classes[ind] if ind < len(tokens) else \"O\"\n",
    "                type_nxt = tag_nxt.split(\"-\")[-1]\n",
    "        else:  # I-tag\n",
    "            name, start, end = tokens[ind]\n",
    "            ind += 1\n",
    "        # Print entity and continue\n",
    "        offset = f\"{start}-{end}\"\n",
    "        txt = f\"{id}|{offset}|{name}|{type}\\n\"\n",
    "        outf.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_1'></a>\n",
    "## Evaluator output for Logistic Regression and Devel set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Train data-set \n",
    "WRONG! We present the output of all the model, where the CRF model, the best, is given with the minimal features to obtain the F1 score of 0.6 corresponding to Goal 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: ML \n",
    "\n",
    "SCORES FOR the file: task9.2_MLLR.txt\n",
    "Gold Dataset: /Devel\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp|fp|fn|total|prec|recall|F1\n",
    "252|155|232|484|0.6192|0.5207|0.5657\n",
    "\n",
    "\n",
    "Detection and Classification of DDI\n",
    "tp|fp|fn|total|prec|recall|F1\n",
    "190|217|294|484|0.4668|0.3926|0.4265\n",
    "\n",
    "\n",
    "\n",
    "##SCORES FOR DDI TYPE\n",
    "\n",
    "Scores for ddi with type mechanism\n",
    "tp|fp|fn|total|prec|recall|F1\n",
    "54|85|147|201|0.3885|0.2687|0.3176\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp|fp|fn|total|prec|recall|F1\n",
    "89|105|73|162|0.4588|0.5494|0.5\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp|fp|fn|total|prec|recall|F1\n",
    "--|--|--|---|------|------\n",
    "45|24|74|119|0.6522|0.3782|0.4787\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp|fp|fn|total|prec|recall|F1\n",
    "2|3|0|2|0.4|1|0.5714\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES:\n",
    "P|R|F1\n",
    "|0.4749|0.549|0.5093"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRONG!!! We see **drug** and **brand** have high F1 scores over **0.7** for all three models, while only **CRF** and **MaxEnt** models have high score for **group**, meaning our features capture well this types of entities.\n",
    "\n",
    "In contrast, the **CRF** model has much lower scores of **0.1** for **drug_n** entities, while **RandomForest** and **MaxEnt** models have, increasingly better scores. In spite of this, all models have 0 incorrect predictions on this type.\n",
    "\n",
    "This result indicate us that our features do not characterise well these type *drug_n* entities, while they characterise pretty good other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see **drug** and **group** have F1 scores over 0.7, meaning our rules capture well this types of entities, while for **brand** and, again, **drug_n** we have much lower scores of 0.63 and 0.05. These values inform us that we need to improve the **drug_n** recognition to improve our model. It is the entity with always bad results which makes the average F1 score always fall down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
