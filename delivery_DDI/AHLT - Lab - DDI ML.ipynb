{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Lab - DDI ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricard Monge and Cristina Capdevila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the deliverables for the AHLT Lab DDI Machine Learning assignment.\n",
    "The notebook contains the following sections:\n",
    "@TOREVISE\n",
    "- [Feature extractor function *extract features*](#features), with subset of features function to achieve Goals 3 and 4.\n",
    "- [Classifier function *classifier*](#classifier)\n",
    "- [Output generator function *output entities*](#output)\n",
    "- [Evaluator output for Devel/Test sets for Goal 3.](#output_1)\n",
    "- [Evaluator output for Devel/Test sets for Goal 4.](#output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features'></a>\n",
    "## Feature extractor function *extract_features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve upon the baseline DDI classification we devise a set of features with which we train four different classifiers to detect the interactions.\n",
    "\n",
    "The fours classifiers we have tested are:\n",
    "- **Maximum Entropy classifier** (**MaxEnt**), through its implementation as a command line executable, details in [here](http://users.umiacs.umd.edu/~hal/megam/version0_3/)\n",
    "- **Multi-layer Perceptron Classifier** (**MLP**), through its implementation in *Sklearn* Python package. We have used a unique hidden layer with size of 45.\n",
    "- **Support Vector Classification** (**SVC**), through its implementation in *Sklearn* Python package.\n",
    "- **Logistic Regression** (**LR**), through its implementation in *Sklearn* Python package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! @COMENTAR PER QUIN MODEL ENS HEM DECANTAT (resultats a baix) [table of results].(#table_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! @COMENTAR LES FEATURES QUE HEM TESTEJAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(analysis, entities, e1, e2):\n",
    "    \"\"\"\n",
    "    Extract Features.\n",
    "    Function which receives an analyzed sentence tree, the entities\n",
    "    present in the sentence, and the ids of the two target entities and returns\n",
    "    a list of features to pass to a ML model to predict DDI.\n",
    "    Args:\n",
    "        - analysis: DependencyGraph object instance with sentence parsed\n",
    "            information.\n",
    "        - entities: dictionary of entities indexed by id with offset as value.\n",
    "        - e1: string with id of the first entity to consider.\n",
    "        - e2: string with id of the second entity to consider.\n",
    "    Return:\n",
    "        - feats: list of features extracted from the tree and e1, e2.\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "\n",
    "    # Get entity nodes from tree\n",
    "    n1 = get_entity_node(analysis, entities, e1)\n",
    "    n2 = get_entity_node(analysis, entities, e2)\n",
    "\n",
    "    # Get verb ancestor from entities\n",
    "    v1 = get_verb_ancestor(analysis, n1)\n",
    "    v2 = get_verb_ancestor(analysis, n2)\n",
    "\n",
    "    # Get ancestors nodes list for entity nodes and verb nodes\n",
    "    ance1 = get_ancestors(analysis, n1)\n",
    "    ance2 = get_ancestors(analysis, n2)\n",
    "    ancev1 = get_ancestors(analysis, v1)\n",
    "    ancev2 = get_ancestors(analysis, v2)\n",
    "\n",
    "    # DDI-type characteristic lemmas\n",
    "    advise_lemmas = [\"administer\", \"use\", \"recommend\", \"consider\", \"approach\",\n",
    "                     \"avoid\", \"monitor\", \"advise\", \"require\", \"contraindicate\"]\n",
    "    effect_lemmas = [\"increase\", \"report\", \"potentiate\", \"enhance\", \"decrease\",\n",
    "                     \"include\", \"result\", \"reduce\", \"occur\", \"produce\",\n",
    "                     \"prevent\", \"effect\"]\n",
    "    int_lemmas = [\"interact\", \"interaction\"]\n",
    "    mechanism_lemmas = [\"reduce\", \"increase\", \"decrease\"]\n",
    "    # Mix lemmas\n",
    "    mix_lemmas = list(set(\n",
    "        advise_lemmas + effect_lemmas + int_lemmas + mechanism_lemmas))\n",
    "    # Modal verbs lemmas\n",
    "    modal_vb = [\"can\", \"could\", \"may\", \"might\", \"must\", \"will\", \"would\",\n",
    "                \"shall\", \"should\"]\n",
    "\n",
    "    # Modal verbs and DDI-type lemmas present in sentence\n",
    "    modal_present = check_lemmas(analysis, modal_vb)\n",
    "    lemma_present = check_lemmas(analysis, mix_lemmas)\n",
    "    advise_present = check_lemmas(analysis, advise_lemmas)\n",
    "    effect_present = check_lemmas(analysis, effect_lemmas)\n",
    "    int_present = check_lemmas(analysis, int_lemmas)\n",
    "    mechanism_present = check_lemmas(analysis, effect_lemmas)\n",
    "\n",
    "    # e2<-*-VB is part DDI-type lemmas\n",
    "    advise_v1 = True if v1[\"lemma\"] in advise_lemmas else \"null\"\n",
    "    effect_v1 = True if v1[\"lemma\"] in effect_lemmas else \"null\"\n",
    "    int_v1 = True if v1[\"lemma\"] in int_lemmas else \"null\"\n",
    "    mechanism_v1 = True if v1[\"lemma\"] in mechanism_lemmas else \"null\"\n",
    "    # e2<-*-VB is part DDI-type lemmas\n",
    "    advise_v2 = True if v2[\"lemma\"] in advise_lemmas else \"null\"\n",
    "    effect_v2 = True if v2[\"lemma\"] in effect_lemmas else \"null\"\n",
    "    int_v2 = True if v2[\"lemma\"] in int_lemmas else \"null\"\n",
    "    mechanism_v2 = True if v2[\"lemma\"] in mechanism_lemmas else \"null\"\n",
    "\n",
    "\n",
    "    # Check if entities hang from the same verb\n",
    "    v1_lemma = v1[\"lemma\"]\n",
    "    v2_lemma = v2[\"lemma\"]\n",
    "    v1_equal_v2 = v1 == v2\n",
    "\n",
    "    # Get head dependencies\n",
    "    e1_rel = n1[\"rel\"]\n",
    "    e2_rel = n2[\"rel\"]\n",
    "    v1_rel = v1[\"rel\"]\n",
    "    v2_rel = v2[\"rel\"]\n",
    "\n",
    "    # Get node dependencies\n",
    "    e1_deps = \"_\".join(n1[\"deps\"].keys()) if len(n1[\"deps\"]) else \"null\"\n",
    "    e2_deps = \"_\".join(n2[\"deps\"].keys()) if len(n2[\"deps\"]) else \"null\"\n",
    "    v1_deps = \"_\".join(v1[\"deps\"].keys()) if len(v1[\"deps\"]) else \"null\"\n",
    "    v2_deps = \"_\".join(v2[\"deps\"].keys()) if len(v2[\"deps\"]) else \"null\"\n",
    "    ance1_deps = \"_\".join([a[\"rel\"] for a in ance1]) if len(ance1) else \"null\"\n",
    "    ance2_deps = \"_\".join([a[\"rel\"] for a in ance2]) if len(ance2) else \"null\"\n",
    "\n",
    "    # Get node order\n",
    "    e1_over_e2 = n1 in ance2\n",
    "    v1_over_v2 = v1 in ancev2\n",
    "    v2_over_v1 = v2 in ancev1\n",
    "\n",
    "    # Common ancestor features\n",
    "    common = ([n for n in ance1 if n in ance2] if len(ance1) > len(ance2) else\n",
    "              [n for n in ance2 if n in ance1])\n",
    "    common_rel = common[0][\"rel\"] if len(common) else \"null\"\n",
    "    common_deps = (\"_\".join(common[0][\"deps\"].keys())\n",
    "                   if len(common) and len(common[0][\"deps\"]) else \"null\")\n",
    "    common_tag = common[0][\"tag\"] if len(common) else \"null\"\n",
    "    common_tag = dict_tags[common_tag]\n",
    "    common_dist_root = (len(ance1) - 1 - ance1.index(common[0])\n",
    "                        if len(common) else 99)\n",
    "    common_dist_e1 = ance1.index(common[0]) if len(common) else 99\n",
    "    common_dist_e2 = ance2.index(common[0]) if len(common) else 99\n",
    "\n",
    "    # Common ancestor son's rel for each entity's branch\n",
    "    common_dep11_rel = (\n",
    "        ance1[ance1.index(common[0]) - 1][\"rel\"]\n",
    "        if len(common) and ance1.index(common[0]) > 0 else \"null\")\n",
    "    common_dep12_rel = (\n",
    "        ance1[ance1.index(common[0]) - 2][\"rel\"]\n",
    "        if len(common) and ance1.index(common[0]) > 1 else \"null\")\n",
    "    common_dep13_rel = (\n",
    "        ance1[ance1.index(common[0]) - 3][\"rel\"]\n",
    "        if len(common) and ance1.index(common[0]) > 2 else \"null\")\n",
    "    common_dep21_rel = (\n",
    "        ance2[ance2.index(common[0]) - 1][\"rel\"]\n",
    "        if len(common) and ance2.index(common[0]) > 0 else \"null\")\n",
    "    common_dep22_rel = (\n",
    "        ance2[ance2.index(common[0]) - 2][\"rel\"]\n",
    "        if len(common) and ance2.index(common[0]) > 1 else \"null\")\n",
    "    common_dep23_rel = (\n",
    "        ance2[ance2.index(common[0]) - 3][\"rel\"]\n",
    "        if len(common) and ance2.index(common[0]) > 2 else \"null\")\n",
    "\n",
    "    # Common ancestor son's tag for each entity's branch\n",
    "    common_dep11_tag = (\n",
    "        dict_tags[ance1[ance1.index(common[0]) - 1][\"tag\"]]\n",
    "        if len(common) and ance1.index(common[0]) > 0 else \"null\")\n",
    "\n",
    "    common_dep22_tag = (\n",
    "        dict_tags[ance2[ance2.index(common[0]) - 2][\"tag\"]]\n",
    "        if len(common) and ance2.index(common[0]) > 1 else \"null\")\n",
    "\n",
    "    # Tree address features\n",
    "    # e1<-conj-x<-dobj-VB-nmod->e2\n",
    "    e2_nmod = get_dependency_address(v2, \"nmod\") == n2[\"address\"]\n",
    "    x_dobj = get_dependency_address(v1, \"dobj\")\n",
    "    nx = analysis.nodes[x_dobj] if x_dobj != -1 else v1\n",
    "    e1_conj_dobj = get_dependency_address(nx, \"conj\") == n1[\"address\"]\n",
    "\n",
    "    # NER features\n",
    "        \n",
    "    # Entity lemma features\n",
    "    lemma1 = str(n1[\"lemma\"])\n",
    "    lemma2 = str(n2[\"lemma\"])\n",
    "    \n",
    "    # 3-Prefix/Suffix from lemma\n",
    "    pre3_1 = lemma1[:3].lower()\n",
    "    pre3_2 = lemma2[:3].lower()\n",
    "    suf3_1 = lemma1[-3:].lower()\n",
    "    suf3_2 = lemma2[-3:].lower()\n",
    "    \n",
    "    # Number of capitals in token\n",
    "    capitals2 = sum(i.isupper() for i in lemma2)\n",
    "\n",
    "    # Gather variables\n",
    "    feats = [\n",
    "        modal_present,  \n",
    "        lemma_present,\n",
    "        advise_present,\n",
    "        effect_present,\n",
    "        int_present,\n",
    "        mechanism_present,  \n",
    "        advise_v1,\n",
    "        effect_v1,\n",
    "        int_v1,\n",
    "        mechanism_v1,\n",
    "        advise_v2,  \n",
    "        effect_v2,\n",
    "        int_v2,\n",
    "        mechanism_v2,\n",
    "        v1_equal_v2,\n",
    "        e1_rel,\n",
    "        e2_rel,\n",
    "        v1_rel,\n",
    "        v2_rel,\n",
    "        e1_deps,  \n",
    "        e2_deps,\n",
    "        e1_over_e2,\n",
    "        v1_over_v2,\n",
    "        v2_over_v1,\n",
    "        common_rel,  \n",
    "        common_tag,\n",
    "        common_dist_root,\n",
    "        common_dist_e1,\n",
    "        common_dist_e2,\n",
    "        common_deps, \n",
    "        common_dep11_rel,\n",
    "        common_dep12_rel,\n",
    "        common_dep13_rel,\n",
    "        common_dep21_rel,\n",
    "        common_dep22_rel, \n",
    "        common_dep23_rel,\n",
    "        common_dep11_tag,\n",
    "        common_dep22_tag,\n",
    "        v1_deps,\n",
    "        v2_deps,  \n",
    "        ance1_deps,\n",
    "        ance2_deps,\n",
    "        pre3_1,\n",
    "        pre3_2,\n",
    "        suf3_1,\n",
    "        suf3_2,\n",
    "    ]\n",
    "    # Turn variables f to categorical var_i=f\n",
    "    feats = [f\"var_{i}={f}\" for i, f in enumerate(feats)]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created some auxiliary functions to get the desired features for the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_node(analysis, entities, entity):\n",
    "    \"\"\"\n",
    "    Get Entity Node.\n",
    "    Function which finds the node in the Dependency Tree which corresponds to\n",
    "    the root of the entity.\n",
    "    Args:\n",
    "        - analysis: DependencyTree object instance with sentence analysis.\n",
    "        - entities: dictionary with entity information.\n",
    "        - entity: string with id of entity to get.\n",
    "    Returns:\n",
    "        - node: dictionary with node from DependencyTree.\n",
    "    \"\"\"\n",
    "    # Get nodes list\n",
    "    nodes = [analysis.nodes[k] for k in analysis.nodes]\n",
    "    ents = entities[entity][\"text\"].split()\n",
    "    # Capture possible tree nodes containing or that are contained in entity\n",
    "    possible = sorted(\n",
    "        [node for node in nodes if node[\"word\"] is not None and\n",
    "         any(ent in node[\"word\"] for ent in ents)],\n",
    "        key=lambda x: x[\"head\"])\n",
    "    node = possible[0] if len(possible) else nodes[0]\n",
    "    return node\n",
    "\n",
    "\n",
    "def get_verb_ancestor(analysis, node):\n",
    "    \"\"\"\n",
    "    Get Verb Ancestor.\n",
    "    Function which looks in the node's antecessor nodes inthe analysis tree\n",
    "    until it finds a verb VB, and returns such verb.\n",
    "    Args:\n",
    "        - analysis: DependencyTree object instance with sentence analysis.\n",
    "        - node: dictionary with node to start from.\n",
    "    Return:\n",
    "        - node: dictionary with verb antecessor node from DependencyTree.\n",
    "    \"\"\"\n",
    "    nodes = analysis.nodes\n",
    "    while node[\"tag\"] != \"TOP\" and \"VB\" not in node[\"tag\"]:\n",
    "        node = nodes[node[\"head\"]]\n",
    "        if not node[\"tag\"]:\n",
    "            break\n",
    "    return node\n",
    "\n",
    "\n",
    "def get_dependency_address(node, dependency):\n",
    "    \"\"\"\n",
    "    Get Dependency Address.\n",
    "    Function which returns the address of a given dependency for a given node,\n",
    "    or a non tractable value -1, which always evaluates to False in the\n",
    "    features. To use when extracting features.\n",
    "    Args:\n",
    "        - node: dictionary with node to look dependencies from.\n",
    "        - dependency: string with dependency name to look for in node.\n",
    "    Return:\n",
    "        - _: string with address of found dependency, or -1 if not found.\n",
    "    \"\"\"\n",
    "    dep = node[\"deps\"][dependency]\n",
    "    # If dependency exists, return address\n",
    "    # If dependency does not exist, return non-value\n",
    "    return dep[0] if len(dep) else -1\n",
    "\n",
    "\n",
    "def check_lemmas(analysis, lemmas):\n",
    "    \"\"\"\n",
    "    Check Lemmas.\n",
    "    Function which checks if the words in the sentence contain the given\n",
    "    lemmas. Then returns the tree-higher encountered lemma, or \"null\" if none\n",
    "    found.\n",
    "    Args:\n",
    "        - analysis: DependencyTree object instance with sentence analysis.\n",
    "        - lemmas: list of strings with lemmas to check.\n",
    "    Returns:\n",
    "        - _: string with present lemma or None.\n",
    "    \"\"\"\n",
    "    nds = analysis.nodes\n",
    "    present = [nds[n] for n in nds\n",
    "               if (nds[n][\"word\"] is not None and nds[n][\"lemma\"] in lemmas)]\n",
    "    present = sorted(present, key=lambda x: x[\"head\"])\n",
    "    # return present[0][\"lemma\"] if len(present) else \"null\"\n",
    "    return \"True\" if len(present) else \"False\"\n",
    "\n",
    "\n",
    "def get_ancestors(analysis, node):\n",
    "    \"\"\"\n",
    "    Get Ancestors.\n",
    "    Function which returns the given node's ancestor nodes.\n",
    "    Args:\n",
    "        - analysis: DependencyTree object instance with sentence analysis.\n",
    "        - node: dictionary with node to start from.\n",
    "    Return:\n",
    "        - node: dictionary with verb antecessor node from DependencyTree.\n",
    "    \"\"\"\n",
    "    ancs = []\n",
    "    nds = analysis.nodes\n",
    "    while node[\"tag\"] and node[\"tag\"] != \"TOP\":\n",
    "        ancs.append(node)\n",
    "        node = nds[node[\"head\"]]\n",
    "    return ancs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classifier'></a>\n",
    "## Classifier function *classifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier function takes the generated features for the data and the trained model, according to the **model** parameter and outputs the predictions given by the model. The different prediction formats of each model type are normalized into the same format and finally passed onto the [ouput_features](#output) function.\n",
    "\n",
    "The *classifier* function makes use of an auxliary function (get_features_labels) to extract the features from the input file, attached following the main function's body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, feature_input, model_input, outputfile):\n",
    "    \"\"\"\n",
    "    Classifier.\n",
    "    Function which retrived a trainer model and predicts the output for a given\n",
    "    validation set features file, to print output to another file.\n",
    "    Args:\n",
    "        - model: string with model type to use.\n",
    "        - feature_input: string with filename of the file to extract features\n",
    "            from to validate the model.\n",
    "        - outputfile: string with filename of output file for validation\n",
    "            predictions.\n",
    "    \"\"\"\n",
    "    # Retrieve sentences, entities and feature vectos\n",
    "    ids, x, _ = get_features_labels(feature_input)\n",
    "    if model == \"MaxEnt\":\n",
    "        # MaxEnt classifier flow\n",
    "        megam_features = f\"{tmp_path}/megam_valid_features.dat\"\n",
    "        megam_predictions = f\"{tmp_path}/megam_predictions.dat\"\n",
    "        system(f\"cat {feature_input} | cut -f {feat_col} > \\\n",
    "            {megam_features}\")\n",
    "        # system(f\"cat {feature_input} | cut -f4- > \\\n",
    "        #     {megam_features}\")\n",
    "        system(f\"./{megam} -quiet -nc -nobias -predict {model_input}.megam \\\n",
    "            multiclass {megam_features} > {megam_predictions}\")\n",
    "        with open(megam_predictions, \"r\") as fp:\n",
    "            lines = fp.readlines()\n",
    "        predictions = [line.split(\"\\t\")[0] for line in lines]\n",
    "\n",
    "    elif model == \"MLP\":\n",
    "        # Retrieve model\n",
    "        with open(f\"{model_input}.MLP\", \"rb\") as fp:\n",
    "            model, encoder = pickle.load(fp)\n",
    "        # OneHotEncode variables\n",
    "        x_ = encoder.transform(x)\n",
    "        # Predict classes\n",
    "        predictions = model.predict(x_)\n",
    "\n",
    "    elif model == \"SVC\":\n",
    "        # Retrieve model\n",
    "        with open(f\"{model_input}.SVC\", \"rb\") as fp:\n",
    "            model, encoder = pickle.load(fp)\n",
    "        # OneHotEncode variables\n",
    "        x_ = encoder.transform(x)\n",
    "        # Predict classes\n",
    "        predictions = model.predict(x_)\n",
    "\n",
    "    elif model == \"GBC\":\n",
    "        # Retrieve model\n",
    "        with open(f\"{model_input}.GBC\", \"rb\") as fp:\n",
    "            model, encoder = pickle.load(fp)\n",
    "        # OneHotEncode variables\n",
    "        x_ = encoder.transform(x)\n",
    "        # Predict classes\n",
    "        predictions = model.predict(x_)\n",
    "\n",
    "    elif model == \"LR\":\n",
    "        # Retrieve model\n",
    "        with open(f\"{model_input}.LR\", \"rb\") as fp:\n",
    "            model, encoder = pickle.load(fp)\n",
    "        # OneHotEncode variables\n",
    "        x_ = encoder.transform(x)\n",
    "        # Predict classes\n",
    "        predictions = model.predict(x_)\n",
    "\n",
    "    else:\n",
    "        print(f\"[ERROR] Model {model} not implemented\")\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Ouput entites for each sentence\n",
    "    with open(outputfile, \"w\") as outf:\n",
    "        for (id, id_e1, id_e2), type in zip(ids, predictions):\n",
    "            output_ddi(id, id_e1, id_e2, type, outf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(input):\n",
    "    \"\"\"\n",
    "    Get Features & Labels.\n",
    "    Function which opens the given filename and extracts the feature and label\n",
    "    vectors, togehter with the sentence and pair entities ids.\n",
    "    Args:\n",
    "        - input: string with filename of file to extract features from.\n",
    "    Returns:\n",
    "        - ids: list of lists with sentence id and entity pairs ids.\n",
    "        - feats: list of lists with binary feature vector.\n",
    "        - labels: list of labels for each entity pair, for the trainer to use.\n",
    "    \"\"\"\n",
    "    with open(input, \"r\") as fp:\n",
    "        lines = fp.read()\n",
    "    pairs = [sent.split(\"\\t\") for sent in lines.split(\"\\n\")[:-1]]\n",
    "    ids = []\n",
    "    labels = []\n",
    "    feats = []\n",
    "    for p in pairs:\n",
    "        ids.append((p[0], p[1], p[2]))\n",
    "        labels.append(p[3])\n",
    "        feat = [elem.split(\"=\")[1] for elem in p[4:]]\n",
    "        feats.append(feat)\n",
    "    return ids, feats, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output'></a>\n",
    "## Output generator function *output features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function recieves the token list  **tokens** for each sentence, identified by the **id** parameter, the ids of each entity of the considered pairs (**e1**,**e2**) in the given sentence, their predicted classes **type** of interaction and the list of extracted **features**. Then it outputs the correspondng line to write in the output features file object **outf**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_features(id, e1, e2, type, features, out):\n",
    "    \"\"\"\n",
    "    Output Features.\n",
    "    Function which outputs to the given opened file object the entity pair\n",
    "    specified with the features extracted from their sentence.\n",
    "    Args:\n",
    "        - id: string with sentence id.\n",
    "        - e1: string with id of the first entity to consider.\n",
    "        - e2: string with id of the second entity to consider.\n",
    "        - type: string with gold class of DDI, for use in training.\n",
    "        - features: list of extracted features from sentence tree.\n",
    "        - outf: file object with opened file for writing output features.\n",
    "    \"\"\"\n",
    "    feature_str = \"\\t\".join(features)\n",
    "    txt = f\"{id}\\t{e1}\\t{e2}\\t{type}\\t{feature_str}\\n\"\n",
    "    out.write(txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='table_results'></a>\n",
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model comparison\n",
    "\n",
    "|model|prec|recall|F1|\n",
    "|--|--|--|--|\n",
    "|MaxEnt|0|0|0|\n",
    "|--|--|--|--|\n",
    "|MLP|0|0|0|\n",
    "|--|--|--|--|\n",
    "|SVC|0|0|0|\n",
    "|--|--|--|--|\n",
    "|KNC|0|0|0|\n",
    "|--|--|--|--|\n",
    "|LR|0|0|0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output_1'></a>\n",
    "## Evaluator output for Logistic Regression and Devel set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for the Train data-set \n",
    "WRONG! We present the output of all the model, where the CRF model, the best, is given with the minimal features to obtain the F1 score of 0.6 corresponding to Goal 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCORES FOR THE GROUP: ML \n",
    "\n",
    "Gold Dataset: /Devel\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "\n",
    "|tp|fp|fn|total|prec|recall|F1|\n",
    "|--|--|--|--|--|--|--|\n",
    "|252|155|232|484|0.6192|0.5207|0.5657|\n",
    "\n",
    "\n",
    "Detection and Classification of DDI\n",
    "\n",
    "|tp|fp|fn|total|prec|recall|F1|\n",
    "|--|--|--|--|--|--|--|\n",
    "|190|217|294|484|0.4668|0.3926|0.4265|\n",
    "\n",
    "\n",
    "\n",
    "#### SCORES FOR DDI TYPE\n",
    "\n",
    "Scores for ddi with type mechanism\n",
    "\n",
    "|tp|fp|fn|total|prec|recall|F1|\n",
    "|--|--|--|--|--|--|--|\n",
    "|54|85|147|201|0.3885|0.2687|0.3176|\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "\n",
    "|tp|fp|fn|total|prec|recall|F1|\n",
    "|--|--|--|--|--|--|--|\n",
    "|89|105|73|162|0.4588|0.5494|0.5|\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "\n",
    "|tp|fp|fn|total|prec|recall|F1|\n",
    "|--|--|--|--|--|--|--|\n",
    "|45|24|74|119|0.6522|0.3782|0.4787|\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "\n",
    "|tp|fp|fn|total|prec|recall|F1|\n",
    "|--|--|--|--|--|--|--|\n",
    "|2|3|0|2|0.4|1|0.5714|\n",
    "\n",
    "\n",
    "####Â MACRO-AVERAGE MEASURES:\n",
    "\n",
    "|P|R|F1|\n",
    "|--|--|--|\n",
    "|0.4749|0.549|0.5093|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@ TO COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
